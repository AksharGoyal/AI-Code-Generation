{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build ChatBot on your own data"
      ],
      "metadata": {
        "id": "OxqMtgpWrfpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "o2KB9Grr7-jU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtkyLjB5prEQ",
        "outputId": "c809023c-d9c3-443e-ed70-fb1c513902d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.292-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
            "  Downloading langsmith-0.0.37-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, openai, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.292 langsmith-0.0.37 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JZ2y9bT78RE",
        "outputId": "a54cc501-9bfa-421b-b9b6-131949e14e3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: faiss-cpu, tiktoken\n",
            "Successfully installed faiss-cpu-1.7.4 tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup OpenAI API Key"
      ],
      "metadata": {
        "id": "AzxZLcQ75kJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import openai"
      ],
      "metadata": {
        "id": "V5sZCnEapxXD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_key = getpass(\"Enter your OpenAI API key:\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_key\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myIqVRM5p5MC",
        "outputId": "43f0ccd1-feb6-4f6d-d39c-7cd20b48fa97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![workflow](https://python.langchain.com/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)"
      ],
      "metadata": {
        "id": "FYEwge6I5-da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required Attribute from above workflow"
      ],
      "metadata": {
        "id": "totmTzWf6GJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader #source+load\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings #transform+embed\n",
        "from langchain.vectorstores import FAISS #vector database/stor\n",
        "from langchain.chains import ConversationalRetrievalChain #retrival\n",
        "from langchain.chat_models import ChatOpenAI #chatGpt model\n",
        "from langchain.memory import ConversationBufferMemory #memory"
      ],
      "metadata": {
        "id": "_L4iVUUprbNe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset\n",
        "\n",
        "We will work on the ChatBot for Nutrients, Download it now: [Healthy-Diet](https://www.kaggle.com/datasets/thedevastator/healthy-diet-recipes-a-comprehensive-dataset)\n"
      ],
      "metadata": {
        "id": "FobPFOry7SEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV file using Langchain CSVLoader\n",
        "loader = CSVLoader(file_path=\"All_Diets.csv\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "Tyls7eLs6DVn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Embeddings"
      ],
      "metadata": {
        "id": "A71eFfXh7xE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data to Embeddings  (Vector/Matrix)\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "_tnTAreJ7uZb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store in Vector Database"
      ],
      "metadata": {
        "id": "GR4F2N-J7z0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store this Vector/Matrix in a Vector Database\n",
        "#Vector database is used for semantic search\n",
        "vectorstore = FAISS.from_documents(data, embeddings)"
      ],
      "metadata": {
        "id": "RYLzJmZx72e_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "2qFxQqZc8H-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=0.0,model_name='gpt-3.5-turbo')"
      ],
      "metadata": {
        "id": "VOtRLP9p8JQM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "_XunKoN68c6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ],
      "metadata": {
        "id": "RkZNCT1A8d6k"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Chain\n",
        "\n",
        "Use Vector Database to extract the semantic search to get the similiar relevant result to the given query/prompt"
      ],
      "metadata": {
        "id": "hYoBG4738PUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm = chat,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    memory = memory # for memory => memory_key is required i.e., chat_history\n",
        ")"
      ],
      "metadata": {
        "id": "VBnTRySo8DDm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions"
      ],
      "metadata": {
        "id": "YC5eKu6k87Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"Give me a recommendation for best Keto diet based on American cuisine type\""
      ],
      "metadata": {
        "id": "V-v3Ttsf858q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res1 = chain({\"question\":prompt1})"
      ],
      "metadata": {
        "id": "qMC_GaeU89Fu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res1['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgx59N-R9D9Y",
        "outputId": "3dedec7c-467b-440b-ef5d-2d7181669479"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the available information, the best Keto diet recommendation based on American cuisine type would be the \"Keto Salad recipes\" recipe. It has a moderate amount of protein, low carbs, and a good amount of fat, which aligns with the principles of a Keto diet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = \"What are the all type of Diets available?\""
      ],
      "metadata": {
        "id": "XA8yY0yJ9YtL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res2 = chain({\"question\":prompt2})"
      ],
      "metadata": {
        "id": "Vkd7nGfW9exY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res2['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNMmIQXE9hHF",
        "outputId": "a6ad30aa-e2b4-43fb-c416-7d54c12ce1bb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, there are two types of diets mentioned: keto and mediterranean.\n"
          ]
        }
      ]
    }
  ]
}