{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKDbLKpYNvAk"
   },
   "source": [
    "# Guide to use BERT model using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZnYzyjoSH0F"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dri6P8bCt7Di",
    "outputId": "88f73604-0b74-4706-b177-a74d9e700069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L71c4v7KSMui"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwpCpxYxJ_CE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGwdmELJNtTQ"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xD3otTdScNd"
   },
   "source": [
    "## Example-1: Use BERT Tokenizer and BERT pre-trained models\n",
    "\n",
    "### Load the pre-trained BERT model and its tokenizer.\n",
    "\n",
    "The model and tokenizer should be specifically designed for question-answering tasks:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzehpdEMP4WI",
    "outputId": "5770ee77-d641-4899-d667-72913d115986"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkPq_6noUuKg"
   },
   "source": [
    "### Tokenization:\n",
    "\n",
    "Tokenization is the process of converting raw text into a format suitable for input to the model. For BERT, this involves breaking text into tokens and mapping them to their corresponding IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W53AHFIHVfK0"
   },
   "outputs": [],
   "source": [
    "# Example context\n",
    "context = \"\"\"\n",
    "AI Planet is a global AI community with headquarters in Belgium and India.\n",
    "It started with the vision to make AI education accessible to everyone and build AI for good to solve key challenges of humanity.\n",
    "As part of our community initiatives, we provide free AI and data science courses by industry experts from large tech companies or\n",
    "startups worldwide. Over 300K+ learners from 150+ countries have benefited since our inception in 2020.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFo69pk8VgHK"
   },
   "outputs": [],
   "source": [
    "question = \"What is the vision of AI Planet?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlGg2_vMUqtq"
   },
   "outputs": [],
   "source": [
    "#tokenize\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URsbRlX9V73D"
   },
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EluGzRfAVqPr"
   },
   "outputs": [],
   "source": [
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPMqbjiaV_dT"
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ls8EwwClV2pS",
    "outputId": "33654164-aa8d-4bb8-8dbd-9aa6db1c5418"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-6.2164, -4.6178, -7.4844, -7.4729, -7.8819, -8.5008, -8.3274, -9.4713,\n",
       "         -9.5067, -6.2164, -0.8900, -4.5244, -6.1605, -5.0710, -3.4343, -4.2045,\n",
       "         -5.6482, -7.3996, -6.0562, -7.8513, -5.3013, -8.0156, -4.7465, -6.2164,\n",
       "          2.3201, -1.2765, -1.3671,  3.1735,  1.5344,  5.9816,  6.5448,  1.2525,\n",
       "         -0.4587, -0.9794, -3.6875, -2.2243, -3.2219,  2.1678, -1.9828, -4.5240,\n",
       "         -2.9454,  0.8003, -0.1549, -3.1594, -3.3971, -6.1947, -2.4343, -4.1118,\n",
       "         -5.3309, -6.9898, -8.1613, -5.1530, -6.0295, -5.9806, -8.0785, -3.5412,\n",
       "         -4.1596, -4.5303, -5.6748, -8.4360, -6.4948, -7.5843, -5.4967, -8.4173,\n",
       "         -7.1801, -7.2999, -8.6525, -7.7577, -7.4749, -7.8930, -8.6425, -6.6666,\n",
       "         -8.8628, -7.4559, -7.4648, -5.2379, -7.1483, -8.4080, -8.1202, -6.1591,\n",
       "         -8.1705, -7.4948, -8.5753, -7.3179, -7.7387, -6.0562, -7.3644, -6.8786,\n",
       "         -5.9852, -7.3483, -4.0782, -8.1759, -6.2161]],\n",
       "       grad_fn=<CloneBackward0>), end_logits=tensor([[-1.1940, -5.4294, -7.0476, -8.1381, -7.7240, -7.7449, -8.2318, -7.6105,\n",
       "         -7.7720, -1.1940, -4.9784, -4.1272, -6.9474, -7.0882, -5.5077, -3.6932,\n",
       "         -3.0980, -6.5499, -5.6403, -7.0041, -4.9438, -7.2101, -3.6284, -1.1938,\n",
       "         -5.7182, -4.9916, -5.3961, -4.1791, -2.6486, -2.5010, -2.0811, -2.7683,\n",
       "          0.1799,  0.1584, -2.8163,  5.3606, -3.0140, -2.4939, -0.1126, -3.0470,\n",
       "          4.5243, -2.9455, -2.4059, -3.2216,  1.6337, -3.1933,  6.2993,  4.7524,\n",
       "         -8.0098, -7.2664, -7.6783, -7.5033, -6.2816, -3.6608, -4.7226, -7.2637,\n",
       "         -6.3836, -5.0059, -4.5097, -7.7335, -6.5389, -4.0114, -2.5908, -6.8238,\n",
       "         -6.2123, -3.9447, -7.0729, -6.9573, -6.4008, -4.9341, -7.4264, -6.6831,\n",
       "         -5.6289, -2.9392, -3.1973, -7.3668, -7.1777, -7.1921, -7.0572, -4.0202,\n",
       "         -7.8034, -6.7175, -7.1701, -5.3238, -7.2551, -4.6626, -7.5020, -7.1722,\n",
       "         -5.1047, -7.8293, -3.2500, -3.7046, -1.1951]],\n",
       "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjUY5JlOWIb_"
   },
   "outputs": [],
   "source": [
    "answer_start_index = int(torch.argmax(output.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(torch.argmax(output.end_logits, axis=-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iObMPGwpevYt"
   },
   "source": [
    "- **output.start_logits**: likely represents the output scores or logits generated by the model for the starting position of the answer span in the input passage.\n",
    "-  **output.end_logits** represents the output scores or logits generated by the model for the ending position of the answer span in the input passage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIUmAbBBiWCL"
   },
   "source": [
    "The returned resoibse is unique IDs for the token. Using Tokenization decode every id into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "gnkODyZNRd0I",
    "outputId": "97a2902b-324c-4e67-a90c-450edccfaaa5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'make ai education accessible to everyone and build ai for good to solve key challenges of humanity'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note: \n",
    "\n",
    "The Transformers library ensures compatibility between tokenizers and models. When selecting a model and tokenizer pair, make sure they are compatible and intended for the same architecture. For example, you should use a BERT tokenizer with a BERT model, a GPT-2 tokenizer with a GPT-2 model, and so on.\n",
    "\n",
    "The Hugging Face model hub provides a variety of pre-trained models and their associated tokenizers. You can find the specific model name in the documentation. For instance, if you're using BERT for sequence classification, you might use bert-base-uncased for both the tokenizer and the model.\n",
    "\n",
    "Remember to check the specific model's documentation for usage details related to inputs, outputs, and special features, as they can vary based on the model's architecture and intended task. The Transformers library documentation and Hugging Face's GitHub repository are excellent resources to explore more about different models, tokenizers, and their applications."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
