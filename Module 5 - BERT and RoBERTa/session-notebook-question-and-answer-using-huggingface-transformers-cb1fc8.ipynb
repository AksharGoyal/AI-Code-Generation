{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook used in Live Session"
      ],
      "metadata": {
        "id": "uFGle_hzMCAv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaNNpfyq6v4C"
      },
      "source": [
        "## Packages üì¶ and Basic Setup\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6qTQdN56v4D"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Configuration"
      ],
      "metadata": {
        "id": "0ce-sKCk2tr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384  # The maximum length of a feature (question and context)\n",
        "doc_stride = (128)  # The authorized overlap between two part of the context when splitting"
      ],
      "metadata": {
        "id": "XVK-THhr2vQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGm7uyVZ6v4E"
      },
      "source": [
        "## üíø The Dataset\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80pU2m7Y6v4F"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from datasets import load_dataset\n",
        "\n",
        "datasets = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdDCfqe56v4G"
      },
      "outputs": [],
      "source": [
        "print(datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ul3ByTt6v4H"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRMzHqYL6v4G"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj5ebH8g6v4I"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a\n",
        "    # stride. This results in one example possible giving several features when a context is long,\n",
        "    # each of those features having a context that overlaps a bit the context of the previous\n",
        "    # feature.\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a\n",
        "    # map from a feature to its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original\n",
        "    # context. This will help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what\n",
        "        # is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this\n",
        "        # span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the\n",
        "            # CLS index).\n",
        "            if not (\n",
        "                offsets[token_start_index][0] <= start_char\n",
        "                and offsets[token_end_index][1] >= end_char\n",
        "            ):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the\n",
        "                # answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge\n",
        "                # case).\n",
        "                while (\n",
        "                    token_start_index < len(offsets)\n",
        "                    and offsets[token_start_index][0] <= start_char\n",
        "                ):\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4K-28OQ6v4J"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = datasets.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"train\"].column_names,\n",
        "    num_proc=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVdwWB7B6v4K"
      },
      "outputs": [],
      "source": [
        "train_set = tokenized_datasets[\"train\"].with_format(\"numpy\")[\n",
        "    :\n",
        "]  # Load the whole dataset as a dict of numpy arrays\n",
        "validation_set = tokenized_datasets[\"validation\"].with_format(\"numpy\")[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhIcJgqK6v4K"
      },
      "source": [
        "## ‚úçÔ∏è Model Architecture\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSKfiDR-6v4L"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNczTKcF6v4M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
        "\n",
        "# Optionally uncomment the next line for float16 training\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkyMfX5e6v4M"
      },
      "source": [
        "## üß± + üèó = üè† Training\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po_BPg7K6v4N"
      },
      "outputs": [],
      "source": [
        "model.fit(train_set, validation_data=validation_set, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymup82KI6v4N"
      },
      "source": [
        "## ‚úÖ Inference\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3cZrWyv6v4N"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"Keras is an API designed for human beings, not machines. Keras follows best\n",
        "practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes\n",
        "the number of user actions required for common use cases, and it provides clear &\n",
        "actionable error messages. It also has extensive documentation and developer guides. \"\"\"\n",
        "question = \"What is Keras?\"\n",
        "\n",
        "inputs = tokenizer([context], [question], return_tensors=\"np\")\n",
        "outputs = model(inputs)\n",
        "start_position = tf.argmax(outputs.start_logits, axis=1)\n",
        "end_position = tf.argmax(outputs.end_logits, axis=1)\n",
        "print(int(start_position), int(end_position[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Sq353Oy6v4N"
      },
      "outputs": [],
      "source": [
        "answer = inputs[\"input_ids\"][0, int(start_position) : int(end_position) + 1]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNYkfnm56v4N"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(answer))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credits: [Question Answering with Hugging Face Transformers](https://keras.io/examples/nlp/question_answering/)\n"
      ],
      "metadata": {
        "id": "Q4y0z6_ZL-34"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}